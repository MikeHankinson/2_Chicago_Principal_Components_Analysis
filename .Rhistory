#Get to Know the Data - Distributions
# -------------------------------
plot1 <- athlete_data %>%
ggplot(aes(x = "All Athletes", y = Total.Distance)) +
geom_jitter(width = .025, height = 0, size = 2, alpha = .5, color = "blue") +
labs(x = "", y="Total Distance (m)")
plot2 <- athlete_data %>%
ggplot(aes(x = "All Athletes", y = Acceleration.Load)) +
geom_jitter(width = .025, height = 0, size = 2, alpha = .5, color = "orange") +
labs(x = "", y="Accelaration Load (NA)")
plot3 <- athlete_data %>%
ggplot(aes(x = "All Athletes", y = Speed.Load)) +
geom_jitter(width = .025, height = 0, size = 2, alpha = .5, color = "green") +
labs(x = "", y="Speed Load (NA)")
plot4 <- athlete_data %>%
ggplot(aes(x = "All Athletes", y = Metabolic.Exertion)) +
geom_jitter(width = .025, height = 0, size = 2, alpha = .5, color = "red") +
labs(x = "", y="Metabolic Exertion (J)")
plot5 <- athlete_data %>%
ggplot(aes(x = "All Athletes", y = Sprints.per.Minute)) +
geom_jitter(width = .025, height = 0, size = 2, alpha = .5, color = "violet") +
labs(x = "", y="Sprints per Minute")
plot6 <- athlete_data %>%
ggplot(aes(x = "All Athletes", y = High.Accelerations.per.Minute)) +
geom_jitter(width = .025, height = 0, size = 2, alpha = .5, color = "yellow") +
labs(x = "", y="High Accelerations per Minute")
plot7 <- athlete_data %>%
ggplot(aes(x = "All Athletes", y = Maximum.Speed)) +
geom_jitter(width = .025, height = 0, size = 2, alpha = .5, color = "black") +
labs(x = "", y="Maximum Speed (m/s)")
plot8 <- athlete_data %>%
ggplot(aes(x = "All Athletes", y = Maximum.Acceleration)) +
geom_jitter(width = .025, height = 0, size = 2, alpha = .5, color = "blue") +
labs(x = "", y="Maximum Acceleration (m/s2)")
grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, plot7, plot8)
# Clean and Normalize Data Set
# -------------------------------
#Normalize, 0-1, columns 2-9 of athlete_data and save to kmeans_athlete_data
kmeans_athlete_data <- sapply(athlete_data[,2:9], function(z) (z-min(z))/(max(z)-min(z)))
#Column bind "Athlete" column from data set with normalized columns
#kmeans_athlete_data <- cbind(athlete_data$Athlete, kmeans_athlete_data[,1:8 ])
# **Temporarily removed below after class**
# kmeans_athlete_data <- cbind(kmeans_athlete_data,athlete_data$Athlete)
##Rename first column back to "Athlete"
# colnames(kmeans_athlete_data)[9] <- 'Athlete'
# a. Run k-means for several different numbers of clusters.
# b. Analyze within sum of squares using an elbow plot.
# c. Observe and discuss the size of the clusters.
# -------------------------------
# ***Algorithm Methodology***
# 1.  Scale variables (0-1) to avoid bias created by variables with different scales.
# 2.  Pick k number of clusters and randomly divide observations into those k groups.
# 3.	Calculate the centroid for each cluster.
# 4.	Calculate the Euclidean distance between every observation and every centroid.
#     Assign each observation to the cluster which has the closest centroid.
# 5.	Iterate until the clusters stop changing.
# 6.  Determine number of clusters to use by incorporating the "Elbow" plot.
# 7.  Return centroid values to original scale for better result interpretation.
# 8.  Plot cluster assignments for visual representation
# 9.  Interpret results.
# Model 2-7 clusters
#Create vector to record in-cluster sum-of-squares model run
withinss <- numeric(7)
# Initialize empty 7X9 matrix to record proportion of population in each cluster for each model iteration
size <- matrix(NA,7,9)
# Populate column 1 with number of clusters for each iteration (2 clusters - 8 clusters)
size[,1] <- 2:8
# Set n to equal the number of rows in the kmeans_athlete_data data set
n <- nrow(kmeans_athlete_data)
for(i in 2:8){
name <- paste("km.object.of7", i, sep = ".") #create name based upon the number of cluster
temp <- assign(name, kmeans(kmeans_athlete_data, centers = i, nstart=50)) #Use assign() to perform Kmeans based upon index, i. saves a kmeans object for each loop iteration
withinss[i-1] <- temp$tot.withinss #saves the within-cluster sum-of-squares and the appropriate index
size[i-1, 2:(i+1)] <- round(temp$size/n, 4) #save the cluster size proportions
}
# View Elbow Plot and Size Proportions
# -------------------------------
plot(withinss, type="b", pch=16, xaxt="n",xlab="# of clusters", main="Elbow Plot")
axis(1, at=1:7, labels=2:8)
size
# By Analyzing the chart, we can see that when the number of groups (K)
# reduces from 4 to 3 there is a big increase in the sum of squares,
# bigger than any other previous increase.
# The main purpose is to find a fair number of groups that could explain satisfactorily a considerable part of the data.
# Based upon elbow plot, select and evaluate 3 clusters.
# d. Observe and discuss the centroids of the clusters.
# Recover Centroid Values to Original Scale
# -------------------------------
# --Evaluation of 3 Clusters--
(centroids <- km.object.of7.3$centers)
restored.centers <- data.frame(matrix(0,nrow=3,ncol=8))
names(restored.centers) <- names(athlete_data[2:9]) ### from 2:9
for(i in 1:8){
restored.centers[,i] <- round(centroids[,i]*(max(athlete_data[,i+1])-min(athlete_data[,i+1]))+min(athlete_data[,i+1]),2)
}
table(restored.centers)
restored.centers
# e. Include any plots or visualizations you think might be helpful.
# -------------------------------
# --Evaluation of 3 Clusters--
# Silhouette refers to a method of interpretation and validation of consistency
# within clusters of data.  It is a measure of similarity to its own
# cluster compared to other clusters.
# Silhouette width*....
# Si > 0 means that the observation is well clustered. The closest it is to 1, the best it is clustered.
# Si < 0 means that the observation was placed in the wrong cluster.
# Si = 0 means that the observation is between two clusters.
# *Reference: towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967
# Silhouette Plot
library(cluster)
library(factoextra)
sil <- silhouette(km.object.of7.3$cluster, dist(kmeans_athlete_data))
fviz_silhouette(sil)
# Clustering Interpretation Plot -- Interactive Plot
library(GGally)
library(plotly)
athlete_data$cluster <- as.factor(km.object.of7.3$cluster)
p <- ggparcoord(data = athlete_data, columns = c(2:9), groupColumn = "cluster", scale = "std") + labs(x = "Physical Activity", y = "value (in standard-deviation units)", title = "Clustering")
ggplotly(p)
athlete_data$cluster
#*****Plot from Class****
my.color <- c("forestgreen", "blue", "red")[km.object.of7.3$cluster]
plot(athlete_data$Total.Distance, athlete_data$Sprints.per.Minute, pch=16, col=my.color, main="Distance vs. Sprint Rate")
#Results Table Included at the Bottom along with Hierarchical Clustering Data.
# f. Discuss your findings and interpret results.
# Verify Proper Categorization within Clusters
# -------------------------------
# The silhouette plot provides evidence for 3 clusters since there are
# no negative silhouette widths and all values exceed 0.50.
# The Clustering Interpretation Plot is not useful in this case.
# Primary Component Analysis (PCA) may be useful to cut the number of
# features.
# ___________________________________________________________________________________________
# ___________________________________________________________________________________________
# 2. Hierarchical Clustering
#   -Using the same data set, perform hierarchical clustering to see how the results compare
#    to those of k-means clustering.
#   -Use multiple linkage methods if you think that it is necessary.
#   -Discuss whether your hierarchical clustering results help inform any previous findings
#    from k-means exploration and whether they confirm or contrast this analysis.
# _______________________________
# Install Packages to Load Library "klaR"
# -------------------------------
# install.packages("klaR")
# install.packages("rlang")
# install.packages("labelled")
#Load and display data
# -------------------------------
athlete_data_2 <- read.xlsx("Assignment_3_data.xlsx", sheet="athlete_data")
#Select only the numeric metrics and scale data for clustering
# -------------------------------
athlete_data_2 <- athlete_data_2[,1:9]
athlete_data_2 <- scale(athlete_data_2)
#Perform Hierarchical clustering
# -------------------------------
# ***Algorithm Methodology***
# 1.  Determine linking method (single, average, complete or centroid)
# 2.  Determine distances between ALL observations(Use a nx-n Dissimilarity Matrix)
# 3.	Visualize the dendogram
# 4.	Identify two most similar objects
# 5.	Identify potential cutpoint(s)
# 6.  Perform the cut and save the clusters
# 7.  Interpret results
# Step 1 - Linking Method
# ...............................
# - a. Use complete method in the hclust() function
# - b. Use average method in the hclust() function
# Step 2 - Determine distances between all observations
# ...............................
# - Use dist() function -- dissimilarity matrix
# - Use hclust() to complete hierarchical cluster analysis
#   on the dissimilarity matrix.
# - Save results into hc (hierarchical cluster) variable
hc_complete <- hclust(dist(athlete_data_2),method = "complete")
hc_average <- hclust(dist(athlete_data_2),method = "average")
# - Confirm the linkage method and distance measure
hc_complete$method       # [1] "complete"
hc_complete$dist.method  # [1] "euclidean"
hc_average$method       # [1] "average"
hc_average$dist.method  # [1] "euclidean"
# Step 3 - Visualize the dendogram
# ...............................
# - Visualize the dendrogram
plot(as.dendrogram(hc_complete))
plot(as.dendrogram(hc_average))
# - see the heights where clusters were joined
hc_complete$height      # [1]   60.72892
hc_average$height       # [1]   60.72892
# Step 4 - Identify two most similar athletes
# ...............................
# - Use cutree() function to cut the tree, e.g., as resulting from hclust,
#   into several groups based upon the cut heights.
# - h=60.8 is the height after the first join was made.  This provide the result
#   after first join...and, it demonstrates the 2 most similar athletes in the dataset.
cutree(hc_complete,h=60.8)
# Note for COMPLETE linking method: Each athlete is in its own individual cluster except
# Athletes 217 and 233 are both in cluster 217.  These are the 2 most similar athletes
cutree(hc_average,h=60.8)
# Note for COMPLETE linking method:
# Again, Athletes 217 and 233 are both in cluster 217.  These are the 2 most similar athletes
# Zoom in the Y-axis between 0 and 1, able to visualize the most similar countries.
plot(as.dendrogram(hc_complete),ylim=c(0, 300))
plot(as.dendrogram(hc_average),ylim=c(0,300))
# Due to the large number of joins, it is difficult to visually concentration of joins
# Step 5 - Identify potential cutpoint(s)
# ...............................
plot(as.dendrogram(hc_complete))
abline(h=300,lty=2,col="dodgerblue")
plot(as.dendrogram(hc_average))
abline(h=300,lty=2,col="dodgerblue")
# Step 6 - Perform cut and save groups
# ...............................
(my.cut.complete <- cutree(hc_complete,h=300))
(my.cut.average <- cutree(hc_average,h=300))
# Go through each value of cluster to print the different country names that
# belong to that value.
# This provides the final result.
athlete.groups.complete <- vector("list",max(my.cut.complete))
for(i in 1:length(athlete.groups.complete)){
athlete.groups.complete[[i]] <- names(my.cut.complete)[which(my.cut.complete==i)]
}
athlete.groups.complete
athlete.groups.average <- vector("list",max(my.cut.average))
for(i in 1:length(athlete.groups.average)){
athlete.groups.average[[i]] <- names(my.cut.average)[which(my.cut.average==i)]
}
athlete.groups.average
# For this large number of athletes and features, I do not like the methodology for performing
# cuts, above.
# Therefore, I would like to place athletes in 5 distinct buckets (clusters).
# This is similar to the star rating utilized by college football recruiting sites
# for perspective high school football players.
(my.cut.complete.5clusters <- cutree(hc_complete,k=5, h=NULL))
(my.cut.average.5clusters <- cutree(hc_average,k=5, h=NULL))
athlete.groups.complete.5clusters <- vector("list",max(my.cut.complete.5clusters))
for(i in 1:length(athlete.groups.complete.5clusters)){
athlete.groups.complete.5clusters[[i]] <- names(my.cut.complete.5clusters)[which(my.cut.complete.5clusters==i)]
}
athlete.groups.complete.5clusters
athlete.groups.average.5clusters <- vector("list",max(my.cut.average.5clusters))
for(i in 1:length(athlete.groups.average.5clusters)){
athlete.groups.average.5clusters[[i]] <- names(my.cut.average.5clusters)[which(my.cut.average.5clusters==i)]
}
athlete.groups.average.5clusters
# Step 7 - Results
# ...............................
# Agglomerative Hierarchical Clustering Results for Both
# Complete and Average Linking Methods Together with K-Means Clustering:
#============================================================================
#                      Complete Method       Average Method       K-Means   #
#                      _______________       ______________       _______   #
# Cluster Size 1             201                   459              125     #
# Cluster Size 2             117                    23              169     #
# Cluster Size 3             136                     6              196     #
# Cluster Size 4              30                     1                      #
# Cluster Size 5               6                     1                      #
#                      _______________       ______________       _______   #
# Total Athletes             490                   490              490     #
#============================================================================
# In this analysis, I chose to move from 3 clusters in K-Means to 5 clusters
# using hierarchical cluster.  Therefore, there isn't a direct comparison of data
# across methodologies.
# However, given the high number of features, it might be prudent to
# consolidate using PCA.
# I am surprised at the difference between the agglomerative methods.
# In addition, I would like to spend additional time re-evaluating
# the K-Means method.  I question the likelihood that all 3 clusters
# would contain the same population.  Perhaps a programming mishap.
my.color <- c("forestgreen", "blue", "red")[km.object.of7.3$cluster]
plot(athlete_data$Total.Distance, athlete_data$Sprints.per.Minute, pch=16, col=my.color, main="Distance vs. Sprint Rate")
rm(list = ls())
# =====================================================
# Module 4 Homework - Principal Component Analysis
# Mike Hankinson
# October 10, 2021
# =====================================================
# *****************************************************
# Assignment
# 1.	Perform Principal Component Analysis to analyze the exercise performance variables in the dataset. Include the following steps in your analysis:
#   a.	Produce a correlation matrix of the variables.
#   b.	Perform PCA and analyze the VAF by each factor. Include an elbow plot.
#   c.	Interpret the first loading using loadings plot.
#   d.	Use a bi plot of the first two factors and loadings and/or other visualizations to identify unique individuals.
# 2. Discuss the following:
# .	Do you think dimension reduction was helpful for this dataset?
# .	Can you draw any connections between your results from clustering analysis and PCA?
# .	What are your overall insights regarding these observations and variables
#     after performing multiple unsupervised modeling techniques?
# *****************************************************
# Solution
# Follow the Process:
# 1.	Load and Scale Data
# 2.	Perform PCA
# 3.	Create Covariance and Correlation Matrices
# 4.	Select the Number of Factors
# 5.	a. Plotting Factors and Loadings
#     b. Extract Insights
# 6. Bi plot of Loadings and Component scores
# 7. Discussion
# 1.	Load and Scale Data
library(openxlsx)
dat <- read.xlsx("Assignment_3_data.xlsx")
dim(dat)      # [1] 490   9
head(dat)
# Athlete Total.Distance Acceleration.Load Speed.Load
# 1       1       2574.810          1377.210  1031.6881
# 2       2       2344.107          1281.303   646.6954
# Metabolic.Exertion Sprints.per.Minute
# 1           365.8638           5.703392
# 2           282.9880           6.333377
# High.Accelerations.per.Minute Maximum.Speed
# 1                     0.5190550          7.53
# 2                     1.0775983          7.83
# Maximum.Acceleration
# 1                 4.58
# 2                 5.85
pca.dat <-scale(dat[,2:9])  # Remove 1st column (athlete number) from data set and scale
dim(pca.dat)  # [1] 490   8
# 2.	Perform PCA
athlete.pca <- princomp(pca.dat)
head(athlete.pca)
#        Comp.1       Comp.2        Comp.3       Comp.4       Comp.5        Comp.6
# 1    0.874306031 -1.722311753 -0.1637680374  0.591605013 -0.218925312 -0.2383615512
# 2    0.247416237  1.189354395 -1.5105905447 -0.577412791 -0.866003169 -0.1176195863
#         Comp.7        Comp.8
# 1    0.341293857 -1.762676e-01
# 2    0.100999877  3.626711e-01
# 3.	Create Covariance and Correlation Matrices
cumulative_variation_VAF <- cumsum(athlete.pca$sdev^2/sum(athlete.pca$sdev^2))  # VAF
# Comp.1    Comp.2    Comp.3    Comp.4    Comp.5    Comp.6    Comp.7    Comp.8
# 0.5121450 0.7611089 0.8438478 0.9048600 0.9494903 0.9735950 0.9919680 1.0000000
# 4.	Select the Number of Factors
plot(cumulative_variation_VAF, type="b", pch=16, xlab="# Components", ylab="VAF", main="PCA Elbow Plot", xaxt="n")
axis(1, at=1:8, labels=paste0("PC",1:8))
# Argument could be made for either 2 or 3 principal components
# based upon breaking point within the elbow plot:
#   With 2 Principal Components:
#     - Dimension reduction from 8 to 2
#     - Still accounting for 76.1% of the variance
#   With 3 Principal Components:
#     - Dimension reduction from 8 to 3
#     - Still accounting for 84.4% of the variance
# However, I would like to account for at least 90% variance within the data set.
# Therefore, I will select 4 principal components resulting in the following:
#   With 5 Principal Components:
#     - Dimension reduction from 9 to 4
#     - Still accounting for 90.4% of the variance
# 5.a. Plot Factors and Loadings
# ------------------------------------------
# The key to interpreting our principal components will often be found in the LOADINGS.
# The LOADINGS (Eigen VALUES/VECTORS) are the values that explain how factors are connected to the original variables that we are attempting to analyze.
#     ORIGINAL DATA = FACTORS * transpose[LOADINGS]
# --Component 1 Loading Plot--
plot(athlete.pca$loadings[,1], type="b", xaxt="n", col="dodgerblue", pch=16,
xlab="Variable", ylab="Loadings", main="Loadings for Component 1")
axis(1,1:8, c("TotalDist", "AccelLD", "SpeedLD", "MetExert", "Sprintmin", "Accelmin", "MaxSpeed", "MaxAccel"))
# axis(1,1:8, c("1", "2", "3", "4", "5", "6", "7", "8"))
abline(h=0, lty=2)  # abline() adds line to an existing graph
# Results of Component Loading Plot:
#   Component 1 is positively correlated with the following 6 variables:
#         1. Total Distance
#         2  Acceleration Load
#         3. Speed Load
#         4. Metabolic Exertion
#         7. Maximum Speed
#         8. Maximum Acceleration
#   Component 1 is negatively correlated with the following 2 variables:
#         5. Sprints per Minute
#         6. High Accelerations per Minute
#   An increase in the value of component 1 will
#     - increase variables 1, 2, 3, 4, 7 and 8.
#     - decrease variables 5 and 6.
#   This appears to be an ideal solution to improve a good bit of athlete performance.
# 5.b. Extract Insights
# ------------------------------------------
# I want to study the following athletes:
#     - 5 athletes who have the HIGHEST MAXIMUM SPEED PCA SCORES
# This sorts column 7 ascending
five.athletes.to.study <- sort(athlete.pca$scores[,7])
tail(five.athletes.to.study)
#     174       284       226        59       318       285
# 0.9329706 0.9434466 0.9511316 0.9735648 0.9804337 1.0266568
five.highest.speed <- sort(athlete.pca$scores[,7], index.return=T)$ix[486:490]
dat[five.highest.speed,]
# Athlete Total.Distance Acceleration.Load Speed.Load
# 174     174      2651.1492         1068.3449  1058.1902
# 284     284      3761.1394         1713.9127  1499.6586
# 226     226       914.0313          773.2589   407.6776
# 59       59       343.4050          559.8534   260.9583
# 318     318      3288.2450         1379.6117  1356.6456
# 285     285      3493.6994         1705.5140  1086.7038
# Metabolic.Exertion Sprints.per.Minute
# 174          339.74392           6.045984
# 284          432.43867           6.576628
# 226           80.09827           5.957809
# 59            40.07995           6.350976
# 318          394.92513           5.283015
# 285          299.38815           5.711018
# High.Accelerations.per.Minute Maximum.Speed
# 174                     0.9490760          7.03
# 284                     1.0282222          7.95
# 226                     0.7576669          7.03
# 59                      1.8850593          6.88
# 318                     0.2718949          7.23
# 285                     0.7999550          7.37
# ????????????????????????????????????????????????????????????????????????
# 6. Bi Plot of Loadings and Component Scores
# ------------------------------------------
# Create bi plots containing the five athletes we decided to study.
# Generating a bi plot of loading values for each variable and factor scores
# for observations allows to directly evaluate PCA results, and to understand
# differences between particular athletes.
# By looking at the left (vertical) and bottom (horizontal) axes,
# we can see the component scores for athletes, represented within the graph by
# bold black numbers.
# By looking at the right (vertical) and top (horizontal) axes,
# we can observe the loadings components we are using.
# for each variable, represented within the graph by bold red text and arrows.
# biplot for 10 athletes for components 1 and 2
biplot(athlete.pca$scores[five.highest.speed, c(1,2)], athlete.pca$loadings[,c(1,2)])
# biplot for 10 athletes for components 1 and 3
biplot(athlete.pca$scores[five.highest.speed, c(1,3)], athlete.pca$loadings[,c(1,3)])
# biplot for 10 athletes for components 1 and 4
biplot(athlete.pca$scores[five.highest.speed, c(1,4)], athlete.pca$loadings[,c(1,4)])
# biplot for 10 athletes for components 2 and 3
biplot(athlete.pca$scores[five.highest.speed, c(2,3)], athlete.pca$loadings[,c(2,3)])
# biplot for 10 athletes for components 2 and 4
biplot(athlete.pca$scores[five.highest.speed, c(2,4)], athlete.pca$loadings[,c(2,4)])
# biplot for 10 athletes for components 3 and 4
biplot(athlete.pca$scores[five.highest.speed, c(3,4)], athlete.pca$loadings[,c(3,4)])
# 6. Discussion
# ------------------------------------------
# .	Do you think dimension reduction was helpful for this dataset?
# .	Can you draw any connections between your results from clustering analysis and PCA?
# .	What are your overall insights regarding these observations and variables
#   after performing multiple unsupervised modeling techniques?
# In Modules 3 and 4, we worked on a provided data set containing results on 490
# athletes. The data set contained the following 8 variables:
#         1. Total Distance
#         2  Acceleration Load
#         3. Speed Load
#         4. Metabolic Exertion
#         5. Sprints per Minute
#         6. High Accelerations per Minute
#         7. Maximum Speed
#         8. Maximum Acceleration
# Analysis of this data falls under the umbrella of unsupervised learning.
#
# In Module 3, we utilized both K-Means and Hierarchical clustering techniques
# to group similar athletes into "buckets".  The algorithms for each were similar
# and are presented below:
# ***K-Means Algorithm Methodology***
# 1.  Scale variables (0-1) to avoid bias created by variables with different scales.
# 2.  Pick k number of clusters and randomly divide observations into those k groups.
# 3.	Calculate the centroid for each cluster.
# 4.	Calculate the Euclidean distance between every observation and every centroid.
#     Assign each observation to the cluster which has the closest centroid.
# 5.	Iterate until the clusters stop changing.
# 6.  Determine number of clusters to use by incorporating the "Elbow" plot.
# 7.  Return centroid values to original scale for better result interpretation.
# 8.  Plot cluster assignments for visual representation
# 9.  Interpret results.
# ***Hierarchical Algorithm Methodology***
# 1.  Determine linking method (single, average, complete or centroid)
# 2.  Scale variables
# 3.  Determine distances between ALL observations(Use a nx-n Dissimilarity Matrix)
# 4.	Visualize the dendogram
# 5.	Identify two most similar objects
# 6.	Identify potential cutpoint(s)
# 7.  Perform the cut and save the clusters
# 8.  Interpret results
# Using these techniques, we were able to find commonalities between the athletes
# based upon performance within the 8 features.
# However, a problem occurs with clustering techniques when the data contains
# a high number of features.  With high dimensionality, it is difficult to
# effectively differentiate the distances (between points--athletes) in high dimensional space.
# In order to alleviate this concern, we learned Principal Component Analysis(PCA)
# technique in Module 4.  PCA is used to alleviate the clutter in
# dimensional space.  This technique allows for the reduction in features by
# "converting a set of possibly correlated values into a set of linearly
# uncorrelated factors"*.
# The process for PCA is fairly straightforward, however, interpretaion is
# a bit tougher.
# ***Principal Component Analysis Algorithm***
# 1.	Load and Scale Data
# 2.	Perform PCA
# 3.	Create Covariance and Correlation Matrices
# 4.	Select the Number of Factors
# 5.	a. Plotting Factors and Loadings
#     b. Extract Insights
# 6. Bi plot of Loadings and Component scores
# I wonder if approaching the problem in reverse order would be beneficial.
# That is, perform PCA first followed by clustering.
# In this way, we could alleviate the dimensionality problem up front
# then look for commonalities through clustering.
# * Quote from Module 4 Course Material.
#Functions
# ------------------------------------------
# - var(),cor() - var, cov and cor compute the variance of x and the covariance or
#         correlation of x and y if these are vectors. If x and y are matrices
#         then the covariances (or correlations) between the columns of x and
#         the columns of y are computed
# ?princomp() - princomp performs a principal components analysis on the given numeric
#             data matrix and returns the results as an object of class princomp.
# cumsum() - Cumulative Sums
#            Returns a vector whose elements are the cumulative sums,
#            products, minima or maxima of the elements of the argument.
# %*% - Matrix Multiplication
# t() - Matrix Transpose
# abline() # Add Straight Lines to a current Plot
# c() Combine Values into a Vector or List
#    rm(list = ls())      Removes global environment
